{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/marcelo/miniconda3/envs/nlp\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 40000  # Vocabulary size\n",
    "maxlen = 40  # Maximum length of input sequences\n",
    "dim_model = 128  # Dimension of the model\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 512  # Dimension of the feed-forward layer\n",
    "num_blocks = 2  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "#clearned_corpus = f'./data/small_20000t_170Ksamples_clearned_corpus.txt.gz'\n",
    "#file_word_dict = './data/small_20000t_170Ksamples_word_dict.pickle'\n",
    "#file_count_words = './data/small_20000t_170Ksamples_count_words.parquet'\n",
    "#model_filename = './models/small_20000t_170Ksamples_nano_gpt_by_marcelo.keras'\n",
    "\n",
    "clearned_corpus = f'./data/clearned_corpus.txt.gz'\n",
    "file_word_dict = './data/word_dict.pickle'\n",
    "file_count_words = './data/count_words.parquet'\n",
    "model_filename = './models/nano_gpt_by_marcelo.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_words = pd.read_parquet(file_count_words, engine='pyarrow')\n",
    "df_count_words.info()\n",
    "vocab = df_count_words.sort_values('count', ascending=False).head(vocab_size-2)['word'].unique().copy()\n",
    "del df_count_words\n",
    "print(vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(clearned_corpus, 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "text_pairs = []\n",
    "for line in lines:    \n",
    "    _split = line.split()\n",
    "    x, y = (' '.join(_split[:-1]), _split[-1])\n",
    "    text_pairs.append((x, y))\n",
    "    \n",
    "del lines    \n",
    "print('Length of pairs:', len(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TextVectorization(\n",
    "    vocabulary=vocab,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen,\n",
    "    standardize=None,\n",
    ")\n",
    "vocab = vectorization.get_vocabulary()\n",
    "print('Vocab Size:', len(vocab))\n",
    "vocab = [str(x) for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization('escultura em barro pintado de um sanfoneiro um dos músicos que integram as bandas de forró caruaru pernambuco o termo forró segundo o filólogo pernambucano evanildo bechara é uma redução de forrobodó que por sua vez')\n",
    "\n",
    "# Test with: small_20000t_170Ksamples_\n",
    "#<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
    "#array([ 1435,     8,  8691,  7607,     2,    10, 11655,    10,    19,\n",
    "#       13004,    11,  8568,    23,  1880,     2,  2207,  4661,  1127,\n",
    "#           4,    91,  2207,    73,     4,  6308,  3455, 19886,     1,\n",
    "#           9,    12, 15187,     2,  8971,    11,    17,    34,   208,\n",
    "#           0,     0,     0,     0])>\n",
    "\n",
    "# Test with: big_40000t_9.8MMsamples_\n",
    "#<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
    "#array([ 3118,     9,  5420,  8115,     2,    11,     1,    11,    20,\n",
    "#        1641,    16,  9655,    26,   438,     2, 10688,  9693,  1132,\n",
    "#           6,   208, 10688,    97,     6, 11580,  5089,     1,     1,\n",
    "#           8,    10,  4351,     2,     1,    16,    17,    30,   309,\n",
    "#           0,     0,     0,     0])> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "#train_pairs, test_pairs = train_test_split(text_pairs, test_size=0.10)\n",
    "num_val_samples = int(0.05 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "print(f'Samples:[{len(text_pairs)}] - Train:[{num_train_samples}] - Val:[{num_val_samples}] - Test:[{len(text_pairs) - num_train_samples - num_val_samples}]')\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = random.choice(text_pairs)\n",
    "print(f'length: {len(pair[0].split())} - text: [{pair[0]}] - vector:\\n{vectorization(pair[0])}')\n",
    "print(f'length: {len(pair[1].split())} - text: [{pair[1]}] - vector: [{vectorization(pair[1])[0]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'[end]' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x, y):\n",
    "    x = vectorization(x)\n",
    "    y = vectorization(y)[:,0]\n",
    "    return x, y \n",
    "\n",
    "def make_dataset(pairs):\n",
    "    x, y = zip(*pairs)\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for final_text in train_ds.take(1).cache():\n",
    "        print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, vocab_size, dim_model, max_len, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_model = dim_model\n",
    "        self.max_len = max_len\n",
    "        #self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        #self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize the token embedding layer\n",
    "        self.token_emb = Embedding(input_dim=self.vocab_size, output_dim=self.dim_model)\n",
    "        # Initialize the positional embedding layer\n",
    "        self.pos_emb = Embedding(input_dim=self.max_len, output_dim=self.dim_model)\n",
    "        # Mark the layer as built\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'dim_model': self.dim_model,\n",
    "            'max_len': self.max_len\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = PositionalEmbedding(vocab_size, dim_model, max_len)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, dim_model, num_heads, ff_dim, dropout)\n",
    "    # Flatten the input\n",
    "    reshape = Reshape((-1, 1))(x)  # Output shape: (None, 80 * 40000)            \n",
    "    flattened = Flatten()(reshape)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(flattened)\n",
    "    #outputs = Reshape((1, vocab_size))(dense)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_model(vocab_size, maxlen, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, test_pairs, max_length=100, temperature=1.0, model = None):\n",
    "        super(TextGeneration, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_pairs = test_pairs        \n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.vocab = self.tokenizer.get_vocabulary()\n",
    "        self.index_word = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "        self.m = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "        prompt, label, text_generated = self.auto_generated_text()\n",
    "        print(f'Label    [{len(label.split()):03}]:[{label}]')\n",
    "        print(f'Prompt   [{len(prompt.split()):03}]:[{prompt}]')        \n",
    "        print(f'Generated[{len(text_generated.split()):03}]:[{text_generated}]')\n",
    "        \n",
    "    def auto_generated_text(self):\n",
    "        prompt, label = random.choice(self.test_pairs)\n",
    "        text_generated = self.generate_text(prompt)\n",
    "        return prompt, label, text_generated\n",
    "        \n",
    "    def generate_text(self, prompt):        \n",
    "        input_eval = self.tokenizer(prompt)\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = prompt.split()\n",
    "        for i in range(self.max_length):\n",
    "            if self.m is None:\n",
    "                predictions = self.model(input_eval)\n",
    "            else:\n",
    "                predictions = self.m(input_eval)\n",
    "            predictions = predictions / self.temperature            \n",
    "            predicted_id = np.argmax(predictions[0])                       \n",
    "            word_predicted = self.index_word[predicted_id]\n",
    "            if word_predicted == '[end]':\n",
    "                break\n",
    "            text_generated.append(word_predicted)\n",
    "            input_eval = vectorization(' '.join(text_generated))\n",
    "            input_eval = tf.expand_dims(input_eval, 0)              \n",
    "\n",
    "        final_text = ' '.join(text_generated)\n",
    "        \n",
    "        return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_filename):\n",
    "    model = keras.models.load_model(model_filename)\n",
    "    print(f'Model loaded from {model_filename}')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='./nano_gpt_by_marcelo.keras',\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch')\n",
    "\n",
    "# Create the TextGeneration callback\n",
    "text_gen_callback = TextGeneration(vectorization, test_pairs=test_pairs, max_length=maxlen, temperature=1)\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=[checkpoint_callback, text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = TextGeneration(vectorization, test_pairs=test_pairs, max_length=maxlen, temperature=1, model=model)\n",
    "for _ in range(5):\n",
    "  prompt, label, generated = text_gen_callback.auto_generated_text()\n",
    "\n",
    "  print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "  print(f'Label[{len(label.split())}]:\\n{label}')\n",
    "  print(f'Generated[{len(generated.split())}]:\\n{generated}')\n",
    "  print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a sequencia de numeros inteiros' #input('Prompt: ')\n",
    "text_gen_callback = TextGeneration(vectorization, test_pairs=None, max_length=maxlen, temperature=1.0, model=model)\n",
    "generated = text_gen_callback.generate_text(prompt)\n",
    "print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')\n",
    "\n",
    "\"\"\"\n",
    "Prompt[5]:\n",
    "a capital de portugal é\n",
    "Generated[39]:\n",
    "a capital de portugal é sede do país que tem chamada de sua pico com a área de frequência de pressão o pico ao do pico ao de direito [UNK] o termo oficial de teologia da o direito de\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
