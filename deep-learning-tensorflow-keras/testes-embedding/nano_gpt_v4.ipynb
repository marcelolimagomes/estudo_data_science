{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "#os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/marcelo/miniconda3/envs/nlp\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738601659.844455   27568 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738601659.847433   27568 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version : 2.18.0\n",
      "Keras Version : 3.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gzip\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras import ops\n",
    "\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 20000  # Vocabulary size\n",
    "maxlen = 20  # Maximum length of input sequences\n",
    "dim_model = 128  # Dimension of the model\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 256  # Dimension of the feed-forward layer\n",
    "num_blocks = 2  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# clearned_corpus = f'./data/small_20000t_170Ksamples_clearned_corpus.txt.gz'\n",
    "# file_word_dict = './data/small_20000t_170Ksamples_word_dict.pickle'\n",
    "# file_count_words = './data/small_20000t_170Ksamples_count_words.parquet'\n",
    "# model_filename = './models/small_20000t_170Ksamples_nano_gpt_by_marcelo.keras'\n",
    "\n",
    "clearned_corpus = f'./data/clearned_corpus_01.txt.gz'\n",
    "file_word_dict = './data/word_dict.pickle'\n",
    "file_count_words = './data/count_words.parquet'\n",
    "model_filename = './models/nano_gpt_v3_by_marcelo.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36437 entries, 0 to 36436\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   word    36437 non-null  object\n",
      " 1   count   36437 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 569.5+ KB\n",
      "(19999,)\n"
     ]
    }
   ],
   "source": [
    "df_count_words = pd.read_parquet(file_count_words, engine='pyarrow')\n",
    "df_count_words.info()\n",
    "vocab = df_count_words.sort_values('count', ascending=False).head(vocab_size - 1)['word'].unique().copy()\n",
    "del df_count_words\n",
    "print(vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>', filters='\\n')\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(clearned_corpus, 'rt') as f:\n",
    "  lines = f.readlines()\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of lines: 500099 Length of sequences: 500099 Length of vectorized sequences: 500099\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vectorized_sequences = pad_sequences(sequences, padding=\"pre\", maxlen=maxlen + 1, dtype='int64') \n",
    "\n",
    "print('Length of lines:', len(lines), 'Length of sequences:', len(sequences), 'Length of vectorized sequences:', len(vectorized_sequences))\n",
    "del sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15, 53, 1, 1, 25, 3, 84, 1, 2838]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"na era pré-moderna , foi a forma meditativa trad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o termo foi utilizado pela primeira vez\n",
      " 7\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   4  76  25 373\n",
      "  40  86 218] (21,)\n"
     ]
    }
   ],
   "source": [
    "print(lines[100], len(lines[100].split()))\n",
    "print(vectorized_sequences[100], vectorized_sequences[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> o termo foi utilizado pela primeira vez']\n",
      "ducados\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sequences_to_texts([vectorized_sequences[100]]))\n",
    "print(tokenizer.index_word[13317])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:[500099] - Train:[450091] - Val:[25004] - Test:[25004]\n"
     ]
    }
   ],
   "source": [
    "#random.shuffle(vectorized_sequences)\n",
    "# train_pairs, test_pairs = train_test_split(text_pairs, test_size=0.10)\n",
    "num_val_samples = int(0.05 * len(lines))\n",
    "num_train_samples = len(lines) - 2 * num_val_samples\n",
    "print(f'Samples:[{len(lines)}] - Train:[{num_train_samples}] - Val:[{num_val_samples}] - Test:[{len(lines) - num_train_samples - num_val_samples}]')\n",
    "train_pairs = vectorized_sequences[:num_train_samples]\n",
    "val_pairs = vectorized_sequences[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = lines[num_train_samples + num_val_samples:]\n",
    "del vectorized_sequences\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(batch_lines):\n",
    "  # Separando features (X) e labels (y)\n",
    "  x = batch_lines[:, :-1]  # Todas as colunas, exceto a última\n",
    "  y = batch_lines[:, 1:]   # A última coluna é o label\n",
    "\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def make_dataset(sequences):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[   0    0    0 ...    7 2980 1018]\n",
      " [   0    0    0 ... 2980 1018    8]\n",
      " [   0    0    0 ... 1018    8   10]\n",
      " ...\n",
      " [   0    0    0 ...   47  275 5340]\n",
      " [   0    0    0 ...  275 5340    1]\n",
      " [   0    0    0 ... 5340    1    1]], shape=(128, 20), dtype=int64)\n",
      "y: tf.Tensor(\n",
      "[[   0    0    0 ... 2980 1018    8]\n",
      " [   0    0    0 ... 1018    8   10]\n",
      " [   0    0    0 ...    8   10   18]\n",
      " ...\n",
      " [   0    0    0 ...  275 5340    1]\n",
      " [   0    0    0 ... 5340    1    1]\n",
      " [   0    0    4 ...    1    1   54]], shape=(128, 20), dtype=int64)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "  for x, y in train_ds.take(1).cache():\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "  def __init__(self, vocab_size, dim_model, max_len, **kwargs):\n",
    "    super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.dim_model = dim_model\n",
    "    self.max_len = max_len\n",
    "    # self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "    # self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # Initialize the token embedding layer\n",
    "    self.token_emb = Embedding(input_dim=self.vocab_size, output_dim=self.dim_model)\n",
    "    # Initialize the positional embedding layer\n",
    "    self.pos_emb = Embedding(input_dim=self.max_len, output_dim=self.dim_model)\n",
    "    # Mark the layer as built\n",
    "    super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "  def call(self, x):\n",
    "    max_len = tf.shape(x)[-1]\n",
    "    positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "    positions = self.pos_emb(positions)\n",
    "    x = self.token_emb(x)\n",
    "    return x + positions\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(PositionalEmbedding, self).get_config()\n",
    "    config.update({\n",
    "        'vocab_size': self.vocab_size,\n",
    "        'dim_model': self.dim_model,\n",
    "        'max_len': self.max_len\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "  x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "  x = Dropout(dropout)(x)\n",
    "  x = LayerNormalization(epsilon=1e-6)(x)\n",
    "  res = x + inputs\n",
    "\n",
    "  x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "  x = Dropout(dropout)(x)\n",
    "  x = Dense(inputs.shape[-1])(x)\n",
    "  x = LayerNormalization(epsilon=1e-6)(x)\n",
    "  return x + res\n",
    "\n",
    "\n",
    "def build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "  inputs = Input(shape=(max_len,))\n",
    "  x = PositionalEmbedding(vocab_size, dim_model, max_len)(inputs)\n",
    "  for _ in range(num_blocks):\n",
    "    x = transformer_block(x, dim_model, num_heads, ff_dim, dropout)\n",
    "  # Flatten the input\n",
    "  # reshape = Reshape((-1, 1))(x)  # Output shape: (None, 80 * 40000)\n",
    "  # flattened = Flatten()(reshape)\n",
    "  outputs = Dense(vocab_size)(x)\n",
    "  return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, tokenizer: Tokenizer, test_pairs, top_k = 10, max_length=100, temperature=1.0, model=None):\n",
    "    super(TextGeneration, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.test_pairs = test_pairs\n",
    "    self.max_length = max_length\n",
    "    self.temperature = temperature\n",
    "    self.m = model\n",
    "    self.k = top_k\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "    prompt, label, text_generated = self.auto_generated_text()\n",
    "    print(f'Label    [{len(label.split()):03}]:[{label}]')\n",
    "    print(f'Prompt   [{len(prompt.split()):03}]:[{prompt}]')\n",
    "    print(f'Generated[{len(text_generated.split()):03}]:[{text_generated}]')\n",
    "\n",
    "  def sample_from(self, logits):\n",
    "      logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "      indices = np.asarray(indices).astype(\"int32\")\n",
    "      preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "      preds = np.asarray(preds).astype(\"float32\")\n",
    "      return np.random.choice(indices[0], p=preds[0])\n",
    "\n",
    "  def auto_generated_text(self):\n",
    "    prompt = random.choice(self.test_pairs)\n",
    "    split = prompt.split()\n",
    "    if len(split) > 20:\n",
    "      split = split[:20]\n",
    "    prompt = ' '.join(split[:-1])\n",
    "    label = ' '.join(split[1:])\n",
    "    text_generated = self.generate_text(prompt)\n",
    "    return prompt, label, text_generated\n",
    "\n",
    "  def generate_text(self, prompt):\n",
    "    input_eval = self.tokenizer.texts_to_sequences([prompt])\n",
    "    input_eval = pad_sequences(input_eval, maxlen=self.max_length, padding='pre')\n",
    "\n",
    "    text_generated = prompt.split()\n",
    "    count_it = 0  # Counter for iterations\n",
    "    #for _ in range(len(prompt.split()) - 1, maxlen):\n",
    "    for _ in range(maxlen):\n",
    "      count_it += 1\n",
    "      if self.m is None:\n",
    "        predictions = self.model(input_eval)\n",
    "      else:\n",
    "        predictions = self.m(input_eval)\n",
    "      predictions = predictions / self.temperature\n",
    "      token = self.sample_from(predictions[:, -1, :])\n",
    "      word_predicted = self.tokenizer.index_word.get(token, ' ')\n",
    "\n",
    "      if word_predicted == '[eos]':\n",
    "        print('>>BREAK WORD<<')\n",
    "        break\n",
    "\n",
    "      text_generated.append(word_predicted)\n",
    "      input_eval = self.tokenizer.texts_to_sequences([' '.join(text_generated)])\n",
    "      input_eval = pad_sequences(input_eval, maxlen=self.max_length, padding='pre')\n",
    "\n",
    "    # Join the generated words into a single string\n",
    "    final_text = ' '.join(text_generated)\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "      final_text = final_text.replace(' ' + i, i)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, maxlen, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./models/nano_gpt_v3_by_marcelo.keras\n",
      "<Functional name=functional, built=True>\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_filename):\n",
    "  model = keras.models.load_model(model_filename, custom_objects={'PositionalEmbedding': PositionalEmbedding})\n",
    "  print(f'Model loaded from {model_filename}')\n",
    "  print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4884 - loss: 4.3788\n",
      "Generating text after epoch: 1\n",
      "Label    [002]:[especificação do]\n",
      "Prompt   [002]:[a especificação]\n",
      "Generated[021]:[a especificação do e em é a em que o que e e da da da da do em da o]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.4885 - loss: 4.3775\n",
      "Epoch 2/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814ms/step - accuracy: 0.5039 - loss: 4.0334\n",
      "Generating text after epoch: 2\n",
      "Label    [016]:[de gelo e neve só algumas algas líquenes briófitas e fungos sobrevivem entre os animais o]\n",
      "Prompt   [016]:[cheio de gelo e neve só algumas algas líquenes briófitas e fungos sobrevivem entre os animais]\n",
      "Generated[016]:[cheio de gelo e neve só algumas algas líquenes briófitas e fungos sobrevivem entre os animais<OOV>]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 817ms/step - accuracy: 0.5039 - loss: 4.0333\n",
      "Epoch 3/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833ms/step - accuracy: 0.5063 - loss: 3.9275\n",
      "Generating text after epoch: 3\n",
      "Label    [011]:[giovanni prandi carlo as ciências das religiões são paulo paulus isbn]\n",
      "Prompt   [011]:[filoramo giovanni prandi carlo as ciências das religiões são paulo paulus]\n",
      "Generated[012]:[filoramo giovanni prandi carlo as ciências das religiões são paulo paulus e<OOV><OOV><OOV><OOV><OOV><OOV><OOV><OOV><OOV>]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 840ms/step - accuracy: 0.5064 - loss: 3.9272\n",
      "Epoch 4/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843ms/step - accuracy: 0.5101 - loss: 3.8053\n",
      "Generating text after epoch: 4\n",
      "Label    [005]:[números quânticos aditivos de antiquarks]\n",
      "Prompt   [005]:[os números quânticos aditivos de]\n",
      "Generated[020]:[os números quânticos aditivos de dezembro de março da março da março de outubro da junho de março de julho]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 851ms/step - accuracy: 0.5101 - loss: 3.8052\n",
      "Epoch 5/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.5067 - loss: 3.8476\n",
      "Generating text after epoch: 5\n",
      "Label    [007]:[densidade populacional por concelho ine populacional por]\n",
      "Prompt   [007]:[ficheiro densidade populacional por concelho ine populacional]\n",
      "Generated[021]:[ficheiro densidade populacional por concelho ine populacional da cidade do rio de satélite da cidade do distrito da cidade da região]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 815ms/step - accuracy: 0.5067 - loss: 3.8472\n",
      "Epoch 6/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859ms/step - accuracy: 0.4965 - loss: 3.8747\n",
      "Generating text after epoch: 6\n",
      "Label    [017]:[da madeira otacílio rock festival festival de música com bandas dos gêneros rock e metal tanto nacionais]\n",
      "Prompt   [017]:[festa da madeira otacílio rock festival festival de música com bandas dos gêneros rock e metal tanto]\n",
      "Generated[021]:[festa da madeira otacílio rock festival festival de música com bandas dos gêneros rock e metal tanto de jesus de abril]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 864ms/step - accuracy: 0.4966 - loss: 3.8743\n",
      "Epoch 7/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840ms/step - accuracy: 0.5171 - loss: 3.5701\n",
      "Generating text after epoch: 7\n",
      "Label    [008]:[primeiros colonizadores portugueses chegaram à região através do]\n",
      "Prompt   [008]:[os primeiros colonizadores portugueses chegaram à região através]\n",
      "Generated[021]:[os primeiros colonizadores portugueses chegaram à região através do oeste ao da região do do brasil do grego do brasil do]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 847ms/step - accuracy: 0.5171 - loss: 3.5705\n",
      "Epoch 8/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823ms/step - accuracy: 0.5194 - loss: 3.6702\n",
      "Generating text after epoch: 8\n",
      "Label    [002]:[é um]\n",
      "Prompt   [002]:[um é]\n",
      "Generated[021]:[um é uma que uma que uma que uma que que que é um é uma longitude ou é uma uma]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 832ms/step - accuracy: 0.5195 - loss: 3.6695\n",
      "Epoch 9/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840ms/step - accuracy: 0.5048 - loss: 3.7184\n",
      "Generating text after epoch: 9\n",
      "Label    [011]:[especificações técnicas do pentium intel pentium imagens do processador e descrições]\n",
      "Prompt   [011]:[intel especificações técnicas do pentium intel pentium imagens do processador e]\n",
      "Generated[021]:[intel especificações técnicas do pentium intel pentium imagens do processador e sistema de criação da comunicação da ordem da número da]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 847ms/step - accuracy: 0.5050 - loss: 3.7176\n",
      "Epoch 10/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828ms/step - accuracy: 0.5057 - loss: 3.7856\n",
      "Generating text after epoch: 10\n",
      "Label    [001]:[do]\n",
      "Prompt   [001]:[destruição]\n",
      "Generated[021]:[destruição do norte do país do brasil no país do brasil na brasil o norte do brasil do estado do brasil]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 838ms/step - accuracy: 0.5058 - loss: 3.7845\n",
      "Epoch 11/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818ms/step - accuracy: 0.5360 - loss: 3.5165\n",
      "Generating text after epoch: 11\n",
      "Label    [011]:[território peruano cobre quilômetros quadrados de área no oeste da américa]\n",
      "Prompt   [011]:[o território peruano cobre quilômetros quadrados de área no oeste da]\n",
      "Generated[021]:[o território peruano cobre quilômetros quadrados de área no oeste da parte da da região da cidade da grande mais grande]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 828ms/step - accuracy: 0.5361 - loss: 3.5159\n",
      "Epoch 12/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.5874 - loss: 3.1239\n",
      "Generating text after epoch: 12\n",
      "Label    [012]:[chegar a sua hora major um velho porco reúne os animais da]\n",
      "Prompt   [012]:[sentindo chegar a sua hora major um velho porco reúne os animais]\n",
      "Generated[014]:[sentindo chegar a sua hora major um velho porco reúne os animais nos seu]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 816ms/step - accuracy: 0.5874 - loss: 3.1237\n",
      "Epoch 13/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843ms/step - accuracy: 0.6185 - loss: 2.8704\n",
      "Generating text after epoch: 13\n",
      "Label    [013]:[à grande quantidade de informação para construir um portal são utilizados sistemas de]\n",
      "Prompt   [013]:[devido à grande quantidade de informação para construir um portal são utilizados sistemas]\n",
      "Generated[016]:[devido à grande quantidade de informação para construir um portal são utilizados sistemas ou sua sua<OOV>]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 847ms/step - accuracy: 0.6186 - loss: 2.8697\n",
      "Epoch 14/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.6769 - loss: 2.3934\n",
      "Generating text after epoch: 14\n",
      "Label    [017]:[parlamento europeu pe obteve sua base com o tratado de paris que instituiu a comunidade europeia do]\n",
      "Prompt   [017]:[o parlamento europeu pe obteve sua base com o tratado de paris que instituiu a comunidade europeia]\n",
      "Generated[017]:[o parlamento europeu pe obteve sua base com o tratado de paris que instituiu a comunidade europeia]\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 860ms/step - accuracy: 0.6769 - loss: 2.3931\n",
      "Epoch 15/50\n",
      "\u001b[1m 61/100\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 939ms/step - accuracy: 0.7062 - loss: 2.2322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      2\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mmodel_filename,\n\u001b[1;32m      3\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m text_gen_callback \u001b[38;5;241m=\u001b[39m TextGeneration(tokenizer, test_pairs\u001b[38;5;241m=\u001b[39mtest_pairs, max_length\u001b[38;5;241m=\u001b[39mmaxlen, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_gen_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/des/estudo_data_science/.env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_filename,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1)\n",
    "\n",
    "model.fit(train_ds, epochs=50, steps_per_epoch=100, callbacks=[checkpoint_callback, text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1, model=model)\n",
    "for _ in range(5):\n",
    "  prompt, label, generated = text_gen_callback.auto_generated_text()\n",
    "\n",
    "  print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "  print(f'Label[{len(label.split())}]:\\n{label}')\n",
    "  print(f'Generated[{len(generated.split())}]:\\n{generated}')\n",
    "  print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a sequencia de numeros inteiros'  \n",
    "vec = tokenizer.texts_to_sequences(prompt.split())\n",
    "vec = tf.convert_to_tensor(vec)\n",
    "pad_size = maxlen - tf.shape(vec)[0]\n",
    "pad_seq = [0] * pad_size.numpy()\n",
    "pad_seq = tf.expand_dims(pad_seq, 0)\n",
    "vec = tf.reshape(vec, [1, -1])\n",
    "vec = tf.concat([pad_seq, vec], axis=1)                    \n",
    "print(vec)\n",
    "\n",
    "\n",
    "predicted = model.predict(vec, verbose=0)\n",
    "predicted.shape, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.convert_to_tensor(predicted)\n",
    "print(predicted[:, -1, :].shape, predicted[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a sequencia de numeros inteiros'  # input('Prompt: ')\n",
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1.0, model=model)\n",
    "generated = text_gen_callback.generate_text(prompt)\n",
    "print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'o espaço expositivo é completamente transparente , fechado com panos de vidro e protegido da incidência solar por painéis de casa barco canoa'\n",
    "split = prompt.split()\n",
    "\n",
    "if len(split) > 20:\n",
    "  split = split[:20]\n",
    "\n",
    "print(f'Texto: [{' '.join(split)}]')\n",
    "\n",
    "prompt = ' '.join(split[:-1])\n",
    "print(f'Prompt: [{prompt}]')\n",
    "\n",
    "label = ' '.join(split[1:])\n",
    "print(f'Label: [{label}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback.auto_generated_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "  result = []\n",
    "  for token in tokens:\n",
    "    result.append(index_word[token.numpy()])\n",
    "  return ' '.join(result)\n",
    "\n",
    "\n",
    "index_word = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "prompt = 'formação estrelar na grande nuvem de magalhães'  # input('Prompt: ')\n",
    "len_prompt = len(prompt.split())\n",
    "print(f'len_prompt: [{len_prompt}] - last_word-1: [{prompt.split()[len_prompt - 1]}] - last_word-2: [{prompt.split()[len_prompt - 2]}]')\n",
    "# Add [bos] token if not already present\n",
    "# if '[bos]' not in prompt:\n",
    "#  prompt = '[bos] ' + prompt\n",
    "\n",
    "# Tokenize the prompt and add batch dimension\n",
    "input_eval = vectorization(prompt)\n",
    "# input_eval = tf.expand_dims(input_eval, 0)  # Shape: (1, sequence_length)\n",
    "\n",
    "# Initialize list to store generated words\n",
    "text_generated = prompt.split()\n",
    "count_it = 0  # Counter for iterations\n",
    "# Generate text iteratively\n",
    "for i in range(len_prompt, maxlen):\n",
    "  count_it += 1\n",
    "  print(f'Prompt       : [{text_generated}]')\n",
    "  print(f'Vector Prompt: [{input_eval}]')\n",
    "  predictions = model(input_eval)  # Use the training model\n",
    "  # Remove batch dimension and apply temperature scaling\n",
    "  predictions = tf.squeeze(predictions, 0)  # Shape: (sequence_length, vocab_size)\n",
    "\n",
    "  predictions = predictions / 1.0\n",
    "  # print(predictions.shape, predictions)\n",
    "  tokens = tf.argmax(predictions, axis=1)\n",
    "  print(f'Predit Tokens: {tokens}')\n",
    "  print(f'Raw Generated: [{detokenize(tokens)}]')\n",
    "  token = tokens[i]\n",
    "  word_predicted = index_word[token.numpy()]\n",
    "  print(f'Iteration    :[{count_it:03}] - Pos:[{i}] - Token:[{token}] - Word pred:[{word_predicted}]')\n",
    "\n",
    "  # Stop if the [eos] token is generated\n",
    "  if word_predicted == '[eos]':\n",
    "    break\n",
    "\n",
    "  # Append the predicted word to the generated text\n",
    "  text_generated.append(word_predicted)\n",
    "\n",
    "  # Update input for the next iteration\n",
    "  input_eval = vectorization(' '.join(text_generated))\n",
    "  # input_eval = tf.expand_dims(input_eval, 0)  # Add batch dimension\n",
    "\n",
    "# Join the generated words into a single string\n",
    "final_text = ' '.join(text_generated)\n",
    "final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
