{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/marcelo/miniconda3/envs/nlp\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 40000  # Vocabulary size\n",
    "maxlen = 40  # Maximum length of input sequences\n",
    "dim_model = 128  # Dimension of the model\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 512  # Dimension of the feed-forward layer\n",
    "num_blocks = 2  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "#clearned_corpus = f'./data/small_20000t_170Ksamples_clearned_corpus.txt.gz'\n",
    "#file_word_dict = './data/small_20000t_170Ksamples_word_dict.pickle'\n",
    "#file_count_words = './data/small_20000t_170Ksamples_count_words.parquet'\n",
    "#model_filename = './models/small_20000t_170Ksamples_nano_gpt_by_marcelo.keras'\n",
    "\n",
    "clearned_corpus = f'./data/clearned_corpus.txt.gz'\n",
    "file_word_dict = './data/word_dict.pickle'\n",
    "file_count_words = './data/count_words.parquet'\n",
    "model_filename = './models/nano_gpt_by_marcelo.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_words = pd.read_parquet(file_count_words, engine='pyarrow')\n",
    "df_count_words.info()\n",
    "vocab = df_count_words.sort_values('count', ascending=False).head(vocab_size-2)['word'].unique().copy()\n",
    "del df_count_words\n",
    "print(vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TextVectorization(\n",
    "    vocabulary=vocab,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen,\n",
    "    standardize=None,\n",
    ")\n",
    "vocab = vectorization.get_vocabulary()\n",
    "print('Vocab Size:', len(vocab))\n",
    "vocab = [str(x) for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization('escultura em barro pintado de um sanfoneiro um dos músicos que integram as bandas de forró caruaru pernambuco o termo forró segundo o filólogo pernambucano evanildo bechara é uma redução de forrobodó que por sua vez')\n",
    "\n",
    "# Test with: small_20000t_170Ksamples_\n",
    "#<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
    "#array([ 1435,     8,  8691,  7607,     2,    10, 11655,    10,    19,\n",
    "#       13004,    11,  8568,    23,  1880,     2,  2207,  4661,  1127,\n",
    "#           4,    91,  2207,    73,     4,  6308,  3455, 19886,     1,\n",
    "#           9,    12, 15187,     2,  8971,    11,    17,    34,   208,\n",
    "#           0,     0,     0,     0])>\n",
    "\n",
    "# Test with: big_40000t_9.8MMsamples_\n",
    "#<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
    "#array([ 3118,     9,  5420,  8115,     2,    11,     1,    11,    20,\n",
    "#        1641,    16,  9655,    26,   438,     2, 10688,  9693,  1132,\n",
    "#           6,   208, 10688,    97,     6, 11580,  5089,     1,     1,\n",
    "#           8,    10,  4351,     2,     1,    16,    17,    30,   309,\n",
    "#           0,     0,     0,     0])> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'[end]' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, vocab_size, dim_model, max_len, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_model = dim_model\n",
    "        self.max_len = max_len\n",
    "        #self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        #self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize the token embedding layer\n",
    "        self.token_emb = Embedding(input_dim=self.vocab_size, output_dim=self.dim_model)\n",
    "        # Initialize the positional embedding layer\n",
    "        self.pos_emb = Embedding(input_dim=self.max_len, output_dim=self.dim_model)\n",
    "        # Mark the layer as built\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'dim_model': self.dim_model,\n",
    "            'max_len': self.max_len\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = PositionalEmbedding(vocab_size, dim_model, max_len)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, dim_model, num_heads, ff_dim, dropout)\n",
    "    # Flatten the input\n",
    "    reshape = Reshape((-1, 1))(x)  # Output shape: (None, 80 * 40000)            \n",
    "    flattened = Flatten()(reshape)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(flattened)\n",
    "    #outputs = Reshape((1, vocab_size))(dense)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_model(vocab_size, maxlen, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, test_pairs, max_length=100, temperature=1.0, model = None):\n",
    "        super(TextGeneration, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_pairs = test_pairs        \n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.vocab = self.tokenizer.get_vocabulary()\n",
    "        self.index_word = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "        self.m = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "        prompt, label, text_generated = self.auto_generated_text()\n",
    "        print(f'Label    [{len(label.split()):03}]:[{label}]')\n",
    "        print(f'Prompt   [{len(prompt.split()):03}]:[{prompt}]')        \n",
    "        print(f'Generated[{len(text_generated.split()):03}]:[{text_generated}]')\n",
    "        \n",
    "    def auto_generated_text(self):\n",
    "        prompt, label = random.choice(self.test_pairs)\n",
    "        text_generated = self.generate_text(prompt)\n",
    "        return prompt, label, text_generated\n",
    "        \n",
    "    def generate_text(self, prompt):        \n",
    "        input_eval = self.tokenizer(prompt)\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = prompt.split()\n",
    "        for i in range(self.max_length):\n",
    "            if self.m is None:\n",
    "                predictions = self.model(input_eval)\n",
    "            else:\n",
    "                predictions = self.m(input_eval)\n",
    "            predictions = predictions / self.temperature            \n",
    "            predicted_id = np.argmax(predictions[0])                       \n",
    "            word_predicted = self.index_word[predicted_id]\n",
    "            if word_predicted == '[end]':\n",
    "                break\n",
    "            text_generated.append(word_predicted)\n",
    "            input_eval = vectorization(' '.join(text_generated))\n",
    "            input_eval = tf.expand_dims(input_eval, 0)              \n",
    "\n",
    "        final_text = ' '.join(text_generated)\n",
    "        \n",
    "        return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_filename):\n",
    "    model = keras.models.load_model(model_filename)\n",
    "    print(f'Model loaded from {model_filename}')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'em bioquímica uma sintase é uma enzima que catalisa um processo de síntese de acordo' #input('Prompt: ')\n",
    "text_gen_callback = TextGeneration(vectorization, test_pairs=None, max_length=maxlen, temperature=1.0, model=model)\n",
    "generated = text_gen_callback.generate_text(prompt)\n",
    "print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
