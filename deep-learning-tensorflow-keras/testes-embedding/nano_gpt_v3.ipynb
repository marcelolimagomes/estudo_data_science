{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/marcelo/miniconda3/envs/nlp\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gzip\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 80000  # Vocabulary size\n",
    "maxlen = 80  # Maximum length of input sequences\n",
    "dim_model = 256  # Dimension of the model\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 512  # Dimension of the feed-forward layer\n",
    "num_blocks = 4  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# clearned_corpus = f'./data/small_20000t_170Ksamples_clearned_corpus.txt.gz'\n",
    "# file_word_dict = './data/small_20000t_170Ksamples_word_dict.pickle'\n",
    "# file_count_words = './data/small_20000t_170Ksamples_count_words.parquet'\n",
    "# model_filename = './models/small_20000t_170Ksamples_nano_gpt_by_marcelo.keras'\n",
    "\n",
    "clearned_corpus = f'./data/clearned_corpus_00.txt.gz'\n",
    "file_word_dict = './data/word_dict.pickle'\n",
    "file_count_words = './data/count_words.parquet'\n",
    "model_filename = './models/nano_gpt_v3_by_marcelo.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_words = pd.read_parquet(file_count_words, engine='pyarrow')\n",
    "df_count_words.info()\n",
    "vocab = df_count_words.sort_values('count', ascending=False).head(vocab_size - 1)['word'].unique().copy()\n",
    "del df_count_words\n",
    "print(vocab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='<OOV>', filters='\\n')\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(clearned_corpus, 'rt') as f:\n",
    "  lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vectorized_sequences = pad_sequences(sequences, padding=\"pre\", maxlen=maxlen + 1, dtype='int64') \n",
    "\n",
    "print('Length of lines:', len(lines), 'Length of sequences:', len(sequences), 'Length of vectorized sequences:', len(vectorized_sequences))\n",
    "del sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([\"na era pré-moderna , foi a forma meditativa trad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lines[100], len(lines[100].split()))\n",
    "print(vectorized_sequences[100], vectorized_sequences[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.sequences_to_texts([vectorized_sequences[100]]))\n",
    "print(tokenizer.index_word[13317])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(vectorized_sequences)\n",
    "# train_pairs, test_pairs = train_test_split(text_pairs, test_size=0.10)\n",
    "num_val_samples = int(0.05 * len(lines))\n",
    "num_train_samples = len(lines) - 2 * num_val_samples\n",
    "print(f'Samples:[{len(lines)}] - Train:[{num_train_samples}] - Val:[{num_val_samples}] - Test:[{len(lines) - num_train_samples - num_val_samples}]')\n",
    "train_pairs = vectorized_sequences[:num_train_samples]\n",
    "val_pairs = vectorized_sequences[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = lines[num_train_samples + num_val_samples:]\n",
    "del vectorized_sequences\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(batch_lines):\n",
    "  # Separando features (X) e labels (y)\n",
    "  x = batch_lines[:, :-1]  # Todas as colunas, exceto a última\n",
    "  y = batch_lines[:, 1:]   # A última coluna é o label\n",
    "\n",
    "  #x.set_shape([None, maxlen])\n",
    "  #y.set_shape([None, maxlen])\n",
    "\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def make_dataset(sequences):\n",
    "  #dataset = tf.data.Dataset.from_tensor_slices([\"brasil o maior país do mundo\", \"são paulo capital\"])\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "  for x, y in train_ds.take(1).cache():\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "  def __init__(self, vocab_size, dim_model, max_len, **kwargs):\n",
    "    super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.dim_model = dim_model\n",
    "    self.max_len = max_len\n",
    "    # self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "    # self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # Initialize the token embedding layer\n",
    "    self.token_emb = Embedding(input_dim=self.vocab_size, output_dim=self.dim_model)\n",
    "    # Initialize the positional embedding layer\n",
    "    self.pos_emb = Embedding(input_dim=self.max_len, output_dim=self.dim_model)\n",
    "    # Mark the layer as built\n",
    "    super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "  def call(self, x):\n",
    "    max_len = tf.shape(x)[-1]\n",
    "    positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "    positions = self.pos_emb(positions)\n",
    "    x = self.token_emb(x)\n",
    "    return x + positions\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(PositionalEmbedding, self).get_config()\n",
    "    config.update({\n",
    "        'vocab_size': self.vocab_size,\n",
    "        'dim_model': self.dim_model,\n",
    "        'max_len': self.max_len\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "  x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "  x = Dropout(dropout)(x)\n",
    "  x = LayerNormalization(epsilon=1e-6)(x)\n",
    "  res = x + inputs\n",
    "\n",
    "  x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "  x = Dropout(dropout)(x)\n",
    "  x = Dense(inputs.shape[-1])(x)\n",
    "  x = LayerNormalization(epsilon=1e-6)(x)\n",
    "  return x + res\n",
    "\n",
    "\n",
    "def build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "  inputs = Input(shape=(max_len,))\n",
    "  x = PositionalEmbedding(vocab_size, dim_model, max_len)(inputs)\n",
    "  for _ in range(num_blocks):\n",
    "    x = transformer_block(x, dim_model, num_heads, ff_dim, dropout)\n",
    "  # Flatten the input\n",
    "  # reshape = Reshape((-1, 1))(x)  # Output shape: (None, 80 * 40000)\n",
    "  # flattened = Flatten()(reshape)\n",
    "  outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "  return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, tokenizer, test_pairs, max_length=100, temperature=1.0, model=None):\n",
    "    super(TextGeneration, self).__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.test_pairs = test_pairs\n",
    "    self.max_length = max_length\n",
    "    self.temperature = temperature\n",
    "    self.m = model\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "    prompt, label, text_generated = self.auto_generated_text()\n",
    "    print(f'Label    [{len(label.split()):03}]:[{label}]')\n",
    "    print(f'Prompt   [{len(prompt.split()):03}]:[{prompt}]')\n",
    "    print(f'Generated[{len(text_generated.split()):03}]:[{text_generated}]')\n",
    "\n",
    "  def auto_generated_text(self):\n",
    "    prompt = random.choice(self.test_pairs)\n",
    "    split = prompt.split()\n",
    "    if len(split) > 20:\n",
    "      split = split[:20]\n",
    "    prompt = ' '.join(split[:-1])\n",
    "    label = ' '.join(split[1:])\n",
    "    text_generated = self.generate_text(prompt)\n",
    "    return prompt, label, text_generated\n",
    "\n",
    "  def detokenize(self, tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "      result.append(self.index_word[token.numpy()])\n",
    "    return ' '.join(result)\n",
    "\n",
    "  def generate_text(self, prompt):\n",
    "    #print(f'Prompt   [{len(prompt.split()):03}]:[{prompt}]')\n",
    "    input_eval = self.tokenizer.texts_to_sequences([prompt])\n",
    "    input_eval = pad_sequences(input_eval, maxlen=self.max_length, padding='pre')\n",
    "\n",
    "    text_generated = prompt.split()\n",
    "    count_it = 0  # Counter for iterations\n",
    "    for _ in range(len(prompt.split()) - 1, maxlen):\n",
    "      count_it += 1\n",
    "      if self.m is None:\n",
    "        predictions = self.model(input_eval)\n",
    "      else:\n",
    "        predictions = self.m(input_eval)\n",
    "      predictions = tf.squeeze(predictions, 0)  # Shape: (sequence_length, vocab_size)\n",
    "      predictions = predictions / self.temperature\n",
    "      token = tf.argmax(predictions[:, -1], axis=0)\n",
    "      word_predicted = self.tokenizer.index_word[token.numpy()]\n",
    "\n",
    "      if word_predicted == '[eos]':\n",
    "        break\n",
    "\n",
    "      text_generated.append(word_predicted)\n",
    "      input_eval = self.tokenizer.texts_to_sequences([' '.join(text_generated)])\n",
    "      input_eval = pad_sequences(input_eval, maxlen=self.max_length, padding='pre')\n",
    "\n",
    "    # Join the generated words into a single string\n",
    "    final_text = ' '.join(text_generated)\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "      final_text = final_text.replace(' ' + i, i)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, maxlen, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_filename):\n",
    "  model = keras.models.load_model(model_filename)\n",
    "  print(f'Model loaded from {model_filename}')\n",
    "  print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_filename,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1)\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=[checkpoint_callback, text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1, model=model)\n",
    "for _ in range(5):\n",
    "  prompt, label, generated = text_gen_callback.auto_generated_text()\n",
    "\n",
    "  print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "  print(f'Label[{len(label.split())}]:\\n{label}')\n",
    "  print(f'Generated[{len(generated.split())}]:\\n{generated}')\n",
    "  print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a sequencia de numeros inteiros'  # input('Prompt: ')\n",
    "text_gen_callback = TextGeneration(tokenizer, test_pairs=test_pairs, max_length=maxlen, temperature=1.0, model=model)\n",
    "generated = text_gen_callback.generate_text(prompt)\n",
    "print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'o espaço expositivo é completamente transparente , fechado com panos de vidro e protegido da incidência solar por painéis de casa barco canoa'\n",
    "split = prompt.split()\n",
    "\n",
    "if len(split) > 20:\n",
    "  split = split[:20]\n",
    "\n",
    "print(f'Texto: [{' '.join(split)}]')\n",
    "\n",
    "prompt = ' '.join(split[:-1])\n",
    "print(f'Prompt: [{prompt}]')\n",
    "\n",
    "label = ' '.join(split[1:])\n",
    "print(f'Label: [{label}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback.auto_generated_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "  result = []\n",
    "  for token in tokens:\n",
    "    result.append(index_word[token.numpy()])\n",
    "  return ' '.join(result)\n",
    "\n",
    "\n",
    "index_word = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "prompt = 'formação estrelar na grande nuvem de magalhães'  # input('Prompt: ')\n",
    "len_prompt = len(prompt.split())\n",
    "print(f'len_prompt: [{len_prompt}] - last_word-1: [{prompt.split()[len_prompt - 1]}] - last_word-2: [{prompt.split()[len_prompt - 2]}]')\n",
    "# Add [bos] token if not already present\n",
    "# if '[bos]' not in prompt:\n",
    "#  prompt = '[bos] ' + prompt\n",
    "\n",
    "# Tokenize the prompt and add batch dimension\n",
    "input_eval = vectorization(prompt)\n",
    "# input_eval = tf.expand_dims(input_eval, 0)  # Shape: (1, sequence_length)\n",
    "\n",
    "# Initialize list to store generated words\n",
    "text_generated = prompt.split()\n",
    "count_it = 0  # Counter for iterations\n",
    "# Generate text iteratively\n",
    "for i in range(len_prompt, maxlen):\n",
    "  count_it += 1\n",
    "  print(f'Prompt       : [{text_generated}]')\n",
    "  print(f'Vector Prompt: [{input_eval}]')\n",
    "  predictions = model(input_eval)  # Use the training model\n",
    "  # Remove batch dimension and apply temperature scaling\n",
    "  predictions = tf.squeeze(predictions, 0)  # Shape: (sequence_length, vocab_size)\n",
    "\n",
    "  predictions = predictions / 1.0\n",
    "  # print(predictions.shape, predictions)\n",
    "  tokens = tf.argmax(predictions, axis=1)\n",
    "  print(f'Predit Tokens: {tokens}')\n",
    "  print(f'Raw Generated: [{detokenize(tokens)}]')\n",
    "  token = tokens[i]\n",
    "  word_predicted = index_word[token.numpy()]\n",
    "  print(f'Iteration    :[{count_it:03}] - Pos:[{i}] - Token:[{token}] - Word pred:[{word_predicted}]')\n",
    "\n",
    "  # Stop if the [eos] token is generated\n",
    "  if word_predicted == '[eos]':\n",
    "    break\n",
    "\n",
    "  # Append the predicted word to the generated text\n",
    "  text_generated.append(word_predicted)\n",
    "\n",
    "  # Update input for the next iteration\n",
    "  input_eval = vectorization(' '.join(text_generated))\n",
    "  # input_eval = tf.expand_dims(input_eval, 0)  # Add batch dimension\n",
    "\n",
    "# Join the generated words into a single string\n",
    "final_text = ' '.join(text_generated)\n",
    "final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
