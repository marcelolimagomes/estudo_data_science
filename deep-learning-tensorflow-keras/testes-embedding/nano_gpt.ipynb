{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version : 2.18.0\n",
      "Keras Version : 3.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 5000  # Vocabulary size\n",
    "max_len = 20  # Maximum length of input sequences\n",
    "d_model = 128  # Dimension of the model\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 512  # Dimension of the feed-forward layer\n",
    "num_blocks = 2  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n"
     ]
    }
   ],
   "source": [
    "filename = './data/clearned_corpus.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "# Define a custom standardization function to add a special token\n",
    "def custom_standardization(input_string):\n",
    "    # Add a special token at the beginning of each sentence\n",
    "    return tf.strings.join([input_string, '[end]'])\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, \n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=max_len,\n",
    "    standardize=None\n",
    "    )\n",
    "vectorization.adapt(lines)\n",
    "print('Vocab Size:', len(vectorization.get_vocabulary()))\n",
    "vocab = [str(x) for x in vectorization.get_vocabulary()]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    _split = line.split()\n",
    "\n",
    "    x, y = (' '.join(_split[:-1]), ' '.join(_split[1:]))\n",
    "    text_pairs.append((x, y))\n",
    "\n",
    "#random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 20 - text: [ainda em criança d afonso casou com a princesa isabel de aragão filha mais velha dos reis católicos isabel i] - vector:\n",
      "[ 227    9 4928   59   57    1   16    4 3036 1004    2  266 1718   31\n",
      "  842   18  615 1923 1004  144]\n",
      "length: 20 - text: [em criança d afonso casou com a princesa isabel de aragão filha mais velha dos reis católicos isabel i [end]] - vector:\n",
      "[   9 4928   59   57    1   16    4 3036 1004    2  266 1718   31  842\n",
      "   18  615 1923 1004  144    3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pair = random.choice(text_pairs)\n",
    "print(f'length: {len(pair[0].split())} - text: [{pair[0]}] - vector:\\n{vectorization(pair[0])}')\n",
    "print(f'length: {len(pair[1].split())} - text: [{pair[1]}] - vector:\\n{vectorization(pair[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x, y):\n",
    "    x = vectorization(x)\n",
    "    y = vectorization(y)\n",
    "    return x, y \n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    x, y = zip(*pairs)\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
      "array([[  15,   20,   24,  155, 3079,    5,  351,   28,    4,    1,    5,\n",
      "         334, 2470,    1,    4,    1, 4656,    1,    8,    4]])>, <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
      "array([[  20,   24,  155, 3079,    5,  351,   28,    4,    1,    5,  334,\n",
      "        2470,    1,    4,    1, 4656,    1,    8,    4,    3]])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 12:23:34.412133: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    for text in train_ds.take(1).cache():\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'd_model': self.d_model,\n",
    "            'max_len': self.max_len\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(vocab_size, max_len, d_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = PositionalEmbedding(vocab_size, d_model, max_len)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, d_model, num_heads, ff_dim, dropout)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_model(vocab_size, max_len, d_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, start_string, max_length=100, temperature=1.0, model = None):\n",
    "        super(TextGeneration, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_string = start_string\n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.vocab = self.tokenizer.get_vocabulary()\n",
    "        self.index_word = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "        self.m = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "        print(self.generate_text())\n",
    "\n",
    "    def generate_text(self):\n",
    "        input_eval = self.tokenizer(self.start_string)\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = []\n",
    "        #self.model.reset_states()\n",
    "        for i in range(self.max_length):\n",
    "            if self.m is None:\n",
    "                predictions = self.model(input_eval)\n",
    "            else:\n",
    "                predictions = self.m(input_eval)\n",
    "\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            predictions = predictions / self.temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "            word_predicted = self.index_word[predicted_id]\n",
    "            if word_predicted == '[end]':\n",
    "                break\n",
    "            text_generated.append(word_predicted)\n",
    "\n",
    "        return self.start_string + ' ' + ' '.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = './nano_gpt_by_marcelo'\n",
    "\n",
    "if os.path.exists(model_filename + '.keras'):\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "    print(f'Model loaded from {model_filename}')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='./nano_gpt_by_marcelo.keras',\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch',\n",
    "        monitor='val_loss')\n",
    "\n",
    "    # Create the TextGeneration callback\n",
    "text_gen_callback = TextGeneration(vectorization, start_string=\"o brasil é um país da américa do sul\", max_length=max_len, temperature=0.5)\n",
    "\n",
    "\n",
    "    # Train the model with the callback\n",
    "model.fit(text_ds, epochs=50, callbacks=[checkpoint_callback, text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = TextGeneration(vectorization, start_string=\"a antropologia do grego anthropos\", max_length=max_len, temperature=0.5, model=model)\n",
    "t = text_gen_callback.generate_text()\n",
    "print(f'Tokens {len(t.split())} - Length: {len(t)}\\n{t}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
