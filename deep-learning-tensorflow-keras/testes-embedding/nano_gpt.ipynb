{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/marcelo/miniconda3/envs/nlp\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.layers import Layer, Embedding, Input, Dense, TextVectorization\n",
    "from keras.models import Model\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')\n",
    "print(f'Keras Version : {keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 40000  # Vocabulary size\n",
    "max_len = 80  # Maximum length of input sequences\n",
    "dim_model = 128  # Dimension of the model\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 512  # Dimension of the feed-forward layer\n",
    "num_blocks = 2  # Number of transformer blocks\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/clearned_corpus.txt.gz'\n",
    "with gzip.open(filename, 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print('Lines:', len(lines))\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, \n",
    "    output_mode=\"int\", \n",
    "    output_sequence_length=max_len,\n",
    "    standardize=None\n",
    "    )\n",
    "vectorization.adapt(lines)\n",
    "print('Vocab Size:', len(vectorization.get_vocabulary()))\n",
    "vocab = [str(x) for x in vectorization.get_vocabulary()]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    _split = line.split()\n",
    "\n",
    "    x, y = (' '.join(_split[:-2]), ' '.join(_split[1:]))\n",
    "    text_pairs.append((x, y))\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "train_pairs, test_pairs = train_test_split(text_pairs, test_size=0.10)\n",
    "#num_val_samples = int(0.15 * len(text_pairs))\n",
    "#num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "#train_pairs = text_pairs[:num_train_samples]\n",
    "#val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "#test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "pair = random.choice(text_pairs)\n",
    "print(f'length: {len(pair[0].split())} - text: [{pair[0]}] - vector:\\n{vectorization(pair[0])}')\n",
    "print(f'length: {len(pair[1].split())} - text: [{pair[1]}] - vector:\\n{vectorization(pair[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x, y):\n",
    "    x = vectorization(x)\n",
    "    y = vectorization(y)\n",
    "    return x, y \n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    x, y = zip(*pairs)\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "#val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for final_text in train_ds.take(1).cache():\n",
    "        print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, vocab_size, dim_model, max_len, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_model = dim_model\n",
    "        self.max_len = max_len\n",
    "        #self.token_emb = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        #self.pos_emb = Embedding(input_dim=max_len, output_dim=d_model)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize the token embedding layer\n",
    "        self.token_emb = Embedding(input_dim=self.vocab_size, output_dim=self.dim_model)\n",
    "        # Initialize the positional embedding layer\n",
    "        self.pos_emb = Embedding(input_dim=self.max_len, output_dim=self.dim_model)\n",
    "        # Mark the layer as built\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'd_model': self.dim_model,\n",
    "            'max_len': self.max_len\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = PositionalEmbedding(vocab_size, dim_model, max_len)(inputs)\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_block(x, dim_model, num_heads, ff_dim, dropout)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_model(vocab_size, max_len, dim_model, num_heads, ff_dim, num_blocks, dropout)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, test_pairs, max_length=100, temperature=1.0, model = None):\n",
    "        super(TextGeneration, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_pairs = test_pairs        \n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.vocab = self.tokenizer.get_vocabulary()\n",
    "        self.index_word = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "        self.m = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'\\nGenerating text after epoch: {epoch + 1}')\n",
    "        prompt, label, text_generated = self.auto_generated_text()\n",
    "        print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "        print(f'Label[{len(label.split())}]:\\n{label}')\n",
    "        print(f'Generated[{len(text_generated.split())}]:\\n{text_generated}')\n",
    "        \n",
    "    def auto_generated_text(self):\n",
    "        prompt, label = random.choice(self.test_pairs)\n",
    "        text_generated = self.generate_text(prompt)\n",
    "        return prompt, label, text_generated\n",
    "        \n",
    "    def generate_text(self, prompt):        \n",
    "        input_eval = self.tokenizer(prompt)\n",
    "        input_eval = tf.expand_dims(input_eval, 0)\n",
    "        text_generated = []\n",
    "        for i in range(self.max_length):\n",
    "            if self.m is None:\n",
    "                predictions = self.model(input_eval)\n",
    "            else:\n",
    "                predictions = self.m(input_eval)\n",
    "            predictions = predictions / self.temperature            \n",
    "            predicted_id = np.argmax(predictions[0, i, :])                       \n",
    "            word_predicted = self.index_word[predicted_id]\n",
    "            if word_predicted == '[end]':\n",
    "                break\n",
    "            text_generated.append(word_predicted)\n",
    "\n",
    "        final_text = ' '.join(text_generated)\n",
    "        \n",
    "        return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = './nano_gpt_by_marcelo.keras'\n",
    "\n",
    "if os.path.exists(model_filename):\n",
    "    model = keras.models.load_model(model_filename)\n",
    "    print(f'Model loaded from {model_filename}')\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='./nano_gpt_by_marcelo.keras',\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch')\n",
    "\n",
    "    # Create the TextGeneration callback\n",
    "text_gen_callback = TextGeneration(vectorization, start_string=\"o brasil é um país da américa do sul\", max_length=max_len, temperature=1)\n",
    "\n",
    "\n",
    "    # Train the model with the callback\n",
    "model.fit(train_ds, epochs=50, callbacks=[checkpoint_callback, text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = TextGeneration(vectorization, test_pairs=test_pairs, max_length=max_len, temperature=1, model=model)\n",
    "prompt, label, generated = text_gen_callback.auto_generated_text()\n",
    "\n",
    "print(f'Prompt[{len(prompt.split())}]:\\n{prompt}')\n",
    "print(f'Label[{len(label.split())}]:\\n{label}')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')\n",
    "\n",
    "generated = text_gen_callback.generate_text('a américa do sul é o continente mais')\n",
    "print(f'Generated[{len(generated.split())}]:\\n{generated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text, label = random.choice(test_pairs)\n",
    "\n",
    "print(f'Input[{len(final_text.split())}]: {final_text}') \n",
    "print(f'Label[{len(label.split())}]: {label}')\n",
    "\n",
    "input_eval = vectorization(final_text)\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "predictions = model(input_eval)\n",
    "print(predictions.shape)\n",
    "predictions\n",
    "\n",
    "#for i in range(0, max_len):  \n",
    "#  input_eval = np.reshape(input_eval, (-1, 1))\n",
    "#  predictions = model(input_eval)  \n",
    "#  predicted_id = np.argmax(predictions[i, 0, :])\n",
    "#  word = index_word[predicted_id]\n",
    "#  print(f'[{i}]: Predicted_id: [{predicted_id}] - Word: [{word}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, max_len):  \n",
    "  input_eval = np.reshape(input_eval, (-1, 1))\n",
    "  predictions = model(input_eval)  \n",
    "  predicted_id = np.argmax(predictions[i, 0, :])\n",
    "  word = index_word[predicted_id]\n",
    "  print(f'[{i}]: Predicted_id: [{predicted_id}] - Word: [{word}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_gen_callback = TextGeneration(vectorization, start_string=\"a literatura latino americana inclui as\", max_length=max_len, temperature=1, model=model)\n",
    "t = text_gen_callback.generate_text()\n",
    "print(f'Tokens {len(t.split())} - Length: {len(t)}\\n{t}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
